# ğŸ¤ AI Voice Assistant â€” 30 Days of AI Voice Agents (Day 14)

An end-to-end **AI Voice Assistant** built as part of the **#30DaysOfAIVoiceAgents** challenge.  
By Day 14, the project has been **refactored, cleaned up, and structured for maintainability**, making it easier to scale and collaborate.  

---

## ğŸ“Œ Project Overview

The AI Voice Assistant:
- Listens to the user through a microphone.  
- Converts speech to text using **Speech-to-Text (STT)** APIs.  
- Sends the transcription to an **LLM** for generating intelligent responses.  
- Converts the LLMâ€™s response into speech using **Text-to-Speech (TTS)**.  
- Plays the audio back to the user in real-time.  
- Features a **modern UI** (introduced on Day 12).  
- Now has **refactored backend code** (Day 14) for better structure and maintainability.  

---

## ğŸ› ï¸ Technologies Used

**Frontend**
- HTML5, CSS3, JavaScript  
- MediaRecorder API for audio capture  
- Fetch API for server communication  

**Backend**
- Python 3.10+  
- FastAPI for API endpoints  
- `uvicorn` as the ASGI server  

**APIs**
- **Speech-to-Text (STT):** AssemblyAI  
- **Large Language Model (LLM):** Google Gemini  
- **Text-to-Speech (TTS):** Murf AI  

**Other**
- **Error Handling** â€“ robust exception handling in backend  
- **Logging** â€“ better debugging and tracking  
- **.env file** â€“ stores API keys securely  
- **Requirements.txt** â€“ project dependencies  

---

## ğŸ—ï¸ Architecture

![Architecture Diagram](images/Architecture.png)

---

## âœ¨ Key Features (Day 14 Updates)

- **ğŸ§¹ Refactored Codebase** â€“ Organized into `services/` folder for STT, LLM, and TTS logic.  
- **ğŸ“¦ Pydantic Models** â€“ Request/Response schemas for API endpoints.  
- **ğŸ›¡ï¸ Error Handling** â€“ Clean `try/except` blocks for graceful failure handling.  
- **ğŸ“ Logging** â€“ Better logs for debugging and monitoring.  
- **ğŸš€ GitHub Integration** â€“ Code cleaned up and pushed to a public repository.  

---

##How to Run

1ï¸âƒ£ Clone the repository

git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>


2ï¸âƒ£ Create virtual environment

python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows


3ï¸âƒ£ Install dependencies

pip install -r requirements.txt


4ï¸âƒ£ Set environment variables
Create a .env file in the project root:

ASSEMBLYAI_API_KEY=your_assemblyai_key
GEMINI_API_KEY=your_gemini_key
MURFAI_API_KEY=your_murfai_key


5ï¸âƒ£ Run the backend

uvicorn main:app --reload


6ï¸âƒ£ Open the frontend

Open index.html in your browser

Allow microphone access

Click ğŸ¤ Start Recording and talk to your assistant

##ğŸ“¦ Folder Structure (Refactored)

â”‚â”€â”€ Day 14/
â”‚   â”‚â”€â”€ index.html           # Frontend UI
â”‚   â”‚â”€â”€ main.py              # FastAPI backend entry point
â”‚   â”‚â”€â”€ requirements.txt     # Dependencies
â”‚   â”‚â”€â”€ .env                 # API keys
â”‚   â”‚â”€â”€ services/            # Refactored services
â”‚   â”‚   â”‚â”€â”€ stt_service.py   # Handles Speech-to-Text
â”‚   â”‚   â”‚â”€â”€ llm_service.py   # Handles Google Gemini
â”‚   â”‚   â”‚â”€â”€ tts_service.py   # Handles Murf AI
â”‚   â”‚
â”‚   â”‚â”€â”€ models/              # Pydantic schemas
â”‚   â”‚   â”‚â”€â”€ request.py
â”‚   â”‚   â”‚â”€â”€ response.py
â”‚
â”‚â”€â”€ images/                  # Project screenshots & architecture diagrams
â”‚   â”‚â”€â”€ Architecture.png
â”‚
â”‚â”€â”€ README.md                # Project documentation


ğŸ“¬ Connect

If youâ€™re building AI voice agents or working on conversational AI, Iâ€™d love to connect!

ğŸ“§ Email: sushantmore1503@example.com
ğŸ”— LinkedIn: www.linkedin.com/in/sushantmore15

#AI #VoiceTech #ConversationalAI #FastAPI #AssemblyAI #GoogleGemini #MurfAI #SpeechToText #TextToSpeech #MachineLearning #Python #VoiceAgents #ErrorHandling #OpenSource #30DaysOfAIVoiceAgents
