# ğŸ¤ AI Voice Assistant â€” 30 Days of AI Voice Agents (Day 1â€“12)

An end-to-end **AI Voice Assistant** built as part of the **#30DaysOfAIVoiceAgents** challenge.  
This project combines **speech recognition, large language models, and text-to-speech** to create a fully interactive conversational agent.

---

## ğŸ“Œ Project Overview

The AI Voice Assistant:
- Listens to the user through a microphone.
- Converts speech to text using **Speech-to-Text (STT)** APIs.
- Sends the transcription to an **LLM** for generating intelligent responses.
- Converts the LLMâ€™s response into speech using **Text-to-Speech (TTS)**.
- Plays the audio back to the user in real-time.
- Features a **revamped UI** (Day 12) for a modern, clean, and interactive experience.

---

## ğŸ› ï¸ Technologies Used

**Frontend**
- HTML5, CSS3, JavaScript
- MediaRecorder API for audio capture
- Fetch API for server communication

**Backend**
- Python 3.10+
- FastAPI for API endpoints
- `uvicorn` as the ASGI server

**APIs**
- **Speech-to-Text (STT):** AssemblyAI
- **Large Language Model (LLM):** Google Gemini
- **Text-to-Speech (TTS):** Murf AI

**Other**
- Fallback audio mechanism for error handling
- Environment variable configuration for API keys

---

## ğŸ—ï¸ Architecture

![Architecture Diagram](images/Archtitecture.png)

## âœ¨ Key Features

- **ğŸ¤ Voice Recording & Processing** â€“ Record user audio directly from the browser and send it to the backend for processing.
- **ğŸ—£ï¸ Speech-to-Text (STT)** â€“ Convert spoken input to text using AssemblyAI API.
- **ğŸ’¬ Conversational AI** â€“ Process user queries with Google Gemini LLM for intelligent responses.
- **ğŸ”Š Text-to-Speech (TTS)** â€“ Generate natural-sounding audio replies with Murf AI.
- **âš¡ Real-time Interaction** â€“ Fast request/response cycle for a smooth conversational experience.
- **ğŸ›¡ï¸ Robust Error Handling** â€“ Gracefully handle STT, LLM, and TTS failures with fallback audio messages.
- **ğŸ¨ Modern UI Design** â€“ Clean, responsive, and interactive frontend with a prominent record button.
- **ğŸ”„ Single Record/Stop Button** â€“ Simplified recording control with dynamic state changes.
- **ğŸ¬ Auto Audio Playback** â€“ Automatically plays AI-generated responses without manual play clicks.
- **ğŸ“‚ Modular Day-wise Code** â€“ Each dayâ€™s folder contains its own `index.html`, backend script, and dependencies for easy tracking.\
  
## How to Run

1ï¸âƒ£ Clone the repository
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Set environment variables
Create a .env file in the project root:

ASSEMBLYAI_API_KEY=your_assemblyai_key
GEMINI_API_KEY=your_gemini_key
MURFAI_API_KEY=your_murfai_key

4ï¸âƒ£ Run the backend
uvicorn main:app --reload
(or python app.py if using Flask)

5ï¸âƒ£ Open the frontend
Open index.html in your browser

Allow microphone access

Click the ğŸ¤ Start Recording button and talk to your assistant


## ğŸ“¦ Folder Structure


â”‚â”€â”€ Day 1/
â”‚ â”‚â”€â”€ index.html # Frontend UI for Day 1
â”‚ â”‚â”€â”€ main.py / app.py # FastAPI or Flask backend for Day 1
â”‚ â”‚â”€â”€ requirements.txt # Python dependencies for Day 1
â”‚

â”‚â”€â”€ Day 2/
â”‚ â”‚â”€â”€ index.html
â”‚ â”‚â”€â”€ main.py / app.py
â”‚ â”‚â”€â”€ requirements.txt
â”‚

â”‚â”€â”€ ...
â”‚

â”‚â”€â”€ Day 12/
â”‚ â”‚â”€â”€ index.html
â”‚ â”‚â”€â”€ main.py / app.py
â”‚ â”‚â”€â”€ requirements.txt
â”‚

â”‚â”€â”€ images/ # Project screenshots & architecture diagrams
â”‚ â”‚â”€â”€ Architecture.png
â”‚

â”‚â”€â”€ README.md # Project documentation


ğŸ“¬ Connect

If youâ€™re building AI voice agents or working on conversational AI, Iâ€™d love to connect!
ğŸ“§ Email: sushantmore1503@example.com
ğŸ”— LinkedIn: www.linkedin.com/in/sushantmore15

#AI #VoiceTech #ConversationalAI #FastAPI #AssemblyAI #GoogleGemini #MurfAI #SpeechToText #TextToSpeech #MachineLearning #Python #VoiceAgents #30DaysOfAIVoiceAgents
